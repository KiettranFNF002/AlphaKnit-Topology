"""
Offline Script: Pack AlphaKnit Dataset into Tensorized Tar Shards

Loads raw JSON and NPY files generated by `gen_phase9b_data.py`.
Pads/samples the point clouds, builds the `src` and `tgt` token sequences,
tensorizes them all, and packs them into `.tar` shards for WebDataset.
"""

import os
import sys
import json
import tarfile
import argparse
import numpy as np
import torch
from tqdm import tqdm

# Add src to python path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))
from alphaknit import config


def fix_point_cloud(pc: np.ndarray, n_points: int) -> np.ndarray:
    """Pad with zeros or randomly subsample to exactly N_POINTS."""
    n = pc.shape[0]
    if n >= n_points:
        # Random subsample
        idx = np.random.choice(n, n_points, replace=False)
        return pc[idx]
    else:
        # Pad with zeros
        pad = np.zeros((n_points - n, 3), dtype=np.float32)
        return np.concatenate([pc, pad], axis=0)


def build_tensor_sample(json_path: str, npy_path: str, max_seq_len: int, n_points: int):
    # Load point cloud
    pc = np.load(npy_path).astype(np.float32)
    pc = fix_point_cloud(pc, n_points)

    # Load sequence
    with open(json_path) as f:
        meta = json.load(f)

    edge_sequence = meta.get("edge_sequence", [])
    
    # Teacher forcing: src = <SOS> + tuples, tgt = tuples + <EOS>
    sos_tuple = [config.SOS_ID, 0, 0]
    eos_tuple = [config.EOS_ID, 0, 0]
    
    src = [sos_tuple] + edge_sequence
    tgt = edge_sequence + [eos_tuple]

    # Truncate
    src = src[:max_seq_len]
    tgt = tgt[:max_seq_len]

    # Pad
    pad_tuple = [config.PAD_ID, 0, 0]
    src = src + [pad_tuple] * (max_seq_len - len(src))
    tgt = tgt + [pad_tuple] * (max_seq_len - len(tgt))

    return {
        "pc": torch.tensor(pc, dtype=torch.float32),
        "src": torch.tensor(src, dtype=torch.long),
        "tgt": torch.tensor(tgt, dtype=torch.long),
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_dir", type=str, required=True, help="Path to JSON/NPY raw dataset")
    parser.add_argument("--output_dir", type=str, required=True, help="Path to output shards directory")
    parser.add_argument("--shard_size", type=int, default=1000, help="Number of samples per tar shard")
    parser.add_argument("--max_seq_len", type=int, default=config.MAX_SEQ_LEN)
    parser.add_argument("--n_points", type=int, default=config.N_POINTS)
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # FUSE workaround: os.listdir often silently truncates on Google Drive if >10k files.
    # We use os.scandir which is an iterator and less prone to memory timeouts.
    files = []
    try:
        print(f"Scanning directory {args.input_dir} (this may take a minute on Google Drive)...")
        for entry in os.scandir(args.input_dir):
            if entry.name.endswith(".json") and entry.name.startswith("sample_"):
                files.append(entry.name[:-5])
    except OSError as e:
        print(f"Error scanning directory: {e}")
        return
        
    files = sorted(files)

    if not files:
        print(f"No samples found in {args.input_dir}")
        return
        
    if len(files) < 50000:
        print(f"WARNING: Only found {len(files)} files. Google Drive FUSE might be truncating the list.")
        print("To fix this, we recommend running this script directly on the raw files before moving them to Drive, or using a cloud bucket.")

    print(f"Packing {len(files)} samples into shards...")

    shard_id = 0
    for i in range(0, len(files), args.shard_size):
        shard_path = os.path.join(args.output_dir, f"shard-{shard_id:04d}.tar")
        subset = files[i : i + args.shard_size]

        with tarfile.open(shard_path, "w") as tar:
            for name in tqdm(subset, desc=f"Writing Shard {shard_id:04d}"):
                
                json_path = os.path.join(args.input_dir, f"{name}.json")
                npy_path = os.path.join(args.input_dir, f"{name}.npy")
                
                # Pre-processing to fully padded tensorized form
                sample = build_tensor_sample(json_path, npy_path, args.max_seq_len, args.n_points)
                
                # Save to temporary pt file
                tmp_path = os.path.join(args.output_dir, f"{name}_tmp.pt")
                torch.save(sample, tmp_path)
                
                # Add to tar archive
                tar.add(tmp_path, arcname=f"{name}.pt")
                
                # Remove temporary file
                os.remove(tmp_path)

        shard_id += 1

    print(f"Done! Created {shard_id} shards in {args.output_dir}")


if __name__ == "__main__":
    main()
