<<<<<<< HEAD
# AlphaKnit ðŸ§¶ - v6.0 The Watchtower

AlphaKnit is a research-grade AI system that translates 3D point clouds into knitting / amigurumi crochet patterns. This version (v6.0) introduces the **"Watchtower" Research Observatory**, focusing on the physics of topological emergence through deep passive telemetry.
=======
# AlphaKnit ðŸ§¶

AlphaKnit is an AI system that translates 3D shapes into knitting / amigurumi crochet patterns. You upload a point cloud or mesh, and the model generates a compilable stitch sequence.
>>>>>>> 6715203057079fa13e1fd3855c710092996e127c

## Architecture

```
3D Point Cloud (NÃ—3)
        â”‚
<<<<<<< HEAD
   PointNetEncoder          â† multi-scale: max-pool + avg-pool + Angular Positional Encoding
        â”‚
   KnittingTransformer       â† Encoder-decoder with Sequential Factorized prediction heads
        â”‚
   Watchtower Observatory    â† Research telemetry (Phase Lag, Latent Portraits, TTF Loss)
        â”‚
   Stitch Tuple Sequence     â† (type, p1_offset, p2_offset)
        â”‚
   KnittingCompiler          â† Validates topology & builds stitch graph
```

## Watchtower Observatory Features

- **Latent Phase Portraits**: Online PCA trajectory visualization of structural embeddings.
- **Phase Lag Monitoring**: Real-time optimizer alignment tracking to detect "Explosions of Choice".
- **Topology Tension Field (TTF)**: Passive bias encouraging structural organization through edge-density penalties.
- **Crystallization Checkpointing**: Automated "Golden Checkpoint" saves at the peak of topological competence.
=======
   PointNetEncoder          â† multi-scale: max-pool + avg-pool concat, BatchNorm
        â”‚
  KnittingTransformer       â† Transformer decoder (d_model=128, 3 layers, 4 heads)
        â”‚
  Token Sequence            â† e.g. "mr_6 sc sc inc sc inc sc eos"
        â”‚
  KnittingCompiler          â† validates topology, builds stitch graph
        â”‚
  ForwardSimulator          â† reconstructs 3D surface for comparison
```

**Phase 8 results (50-epoch training):**
| Metric               | Before | After   |
|----------------------|--------|---------|
| Val Loss             | 0.788  |**0.501**|
| Compile Success Rate | 33.6%  |**92.2%**|
>>>>>>> 6715203057079fa13e1fd3855c710092996e127c

## Project Structure

```
src/alphaknit/
<<<<<<< HEAD
â”œâ”€â”€ model.py            # PointNetEncoder + Factorized KnittingTransformer
â”œâ”€â”€ train.py            # Phase-aware training loop (v6.0 Watchtower integration)
â”œâ”€â”€ research.py         # [NEW] Phase Lag & Latent Phase Portraits
â”œâ”€â”€ metrics.py          # [NEW] Structural Logit Margin & TTF Loss
â”œâ”€â”€ inference.py        # AlphaKnitPredictor â€” wraps model + compiler
â”œâ”€â”€ compiler.py         # KnittingCompiler â€” validates stitch sequences
â”œâ”€â”€ simulator.py        # ForwardSimulator â€” reconstruct mesh from stitch graph
â”œâ”€â”€ tokenizer.py        # Vocabulary & Edge-Action tokenization
â”œâ”€â”€ knitting_dataset.py # WebDataset-optimized loader
â”œâ”€â”€ parser.py           # Stack-based pattern parser
â””â”€â”€ config.py           # Shared constants
=======
â”œâ”€â”€ model.py          # PointNetEncoder + KnittingTransformer (greedy & beam decode)
â”œâ”€â”€ train.py          # Training loop (label smoothing, early stopping, compile logging)
â”œâ”€â”€ inference.py      # AlphaKnitPredictor â€” wraps model + compiler
â”œâ”€â”€ compiler.py       # KnittingCompiler â€” validates stitch sequences
â”œâ”€â”€ simulator.py      # ForwardSimulator â€” reconstruct mesh from stitch graph
â”œâ”€â”€ tokenizer.py      # Vocabulary & token â†” ID conversion
â”œâ”€â”€ knitting_dataset.py  # PyTorch Dataset
â”œâ”€â”€ parser.py         # Stack-based pattern parser
â””â”€â”€ config.py         # Shared constants (vocab, seq lengths, etc.)

scripts/
â”œâ”€â”€ phase8_train.py   # 50-epoch Phase 8 training script
â”œâ”€â”€ eval_phase8.py    # Evaluate greedy vs beam search on test samples
â”œâ”€â”€ generate_data.py  # Synthetic data generation
â””â”€â”€ evaluate.py       # Full evaluation pipeline

checkpoints/
â””â”€â”€ best_model_phase8.pt  # Best model (Epoch 49, val_loss=0.499)

app.py                # Streamlit web demo
>>>>>>> 6715203057079fa13e1fd3855c710092996e127c
```

## Installation

```bash
<<<<<<< HEAD
pip install -r requirements_pc.txt  # Optimized for local PC (CUDA-ready)
```

## Training

AlphaKnit v6.0 uses a "Self-Aware" launch sequence. The system automatically detects your current epoch, selects the appropriate transition phase (Airlock), and chains checkpoints.

```cmd
.\run_pc.bat
```

### Visualization and Telemetry

To visualize the research data and latent phase portraits after training:

```bash
python scripts/plot_v6_telemetry.py --history checkpoints/training_history_phase9b_dev.json
```

This generates:
- `plots/v6_metrics.png`: Logit Margin, Phase Lag, and Accuracy.
- `plots/phase_portrait.png`: The PCA trajectory showing the "Path to Emergence".

## Phase Strategy Evolution

| Version | Focus | Key Technology |
|---|---|---|
| **v4.0** | Stability | Selective Reset + Shock LR |
| **v5.0** | Automation | State-aware Curriculum (PhaseDetector) |
| **v6.0** | Research | Watchtower Observatory (Passive Telemetry) |
=======
pip install -r requirements.txt
```

## Running the Demo

```bash
streamlit run app.py
```

Upload a `.npy` (point cloud) or `.obj` / `.ply` (mesh) file. Adjust the **Beam Width** slider in the sidebar to switch between greedy decoding (fast) and beam search (higher quality).

## Training

```bash
# Fresh Phase 8 training (from scratch â€” new architecture required)
python scripts/phase8_train.py

# Evaluate after training
python scripts/eval_phase8.py --samples 200
```

## Key Phase 8 Improvements

- **Multi-scale PointNet encoder** â€” concatenates max-pool + avg-pool for richer geometry representation
- **Label smoothing (Î±=0.1)** â€” prevents the model from fixating on `sc` (most frequent token)
- **Compile-guided beam search** â€” prunes beams that violate basic stitch topology rules
- **Per-epoch compile success rate** â€” the real metric: % of decoded sequences that pass the compiler
- **Early stopping** (patience = 10) + full checkpoint resume support
>>>>>>> 6715203057079fa13e1fd3855c710092996e127c

## Tests

```bash
python -m pytest tests/
```
